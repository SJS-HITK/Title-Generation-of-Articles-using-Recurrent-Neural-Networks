{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yI12tLz7DfIj"
   },
   "source": [
    "# Training and Evaluation code for Bi-Directional RNN with Attention Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2CceDEgqSs4Z"
   },
   "source": [
    "#### **Downloading and Importing required libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AtyBTrDgMunV"
   },
   "outputs": [],
   "source": [
    "!pip install compress-pickle\n",
    "!pip install rouge\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "56jLGsiZSxkw"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu,SmoothingFunction\n",
    "from rouge import Rouge \n",
    "\n",
    "import collections\n",
    "import compress_pickle as pickle\n",
    "import re\n",
    "import bz2\n",
    "import os \n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import trange,tqdm\n",
    "\n",
    "import spacy\n",
    "import en_core_web_md\n",
    "\n",
    "nltk.download('punkt')\n",
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MHbfcb7WDo4G"
   },
   "source": [
    "#### **Necessary Utility functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ND4lYT2IMunn"
   },
   "outputs": [],
   "source": [
    "# paths for dataset\n",
    "default_path = \"/Training/Model_with_attention/\"\n",
    "dataset_path = \"/Dataset/\"\n",
    "\n",
    "train_article_path = dataset_path + \"abstract.train.bz2\"\n",
    "train_title_path   = dataset_path + \"title.train.bz2\"\n",
    "valid_article_path = dataset_path + \"abstract.valid.bz2\"\n",
    "valid_title_path   = dataset_path + \"title.valid.bz2\"\n",
    "test_article_path = dataset_path + \"abstract.test.bz2\"\n",
    "test_title_path   = dataset_path + \"title.test.bz2\"\n",
    "\n",
    "\n",
    "# removes '.' from abstracts\n",
    "def clean_str(sentence):\n",
    "    sentence = re.sub(\"[#.]+\", \" \", sentence)\n",
    "    return sentence\n",
    "\n",
    "# read zipped files and return as list\n",
    "def get_text_list(data_path, toy,clean=True):\n",
    "    with bz2.open (data_path, \"r\") as f:\n",
    "        if not clean:\n",
    "            return [x.decode().strip() for x in f.readlines()[5000:10000:5]]\n",
    "        if not toy:\n",
    "            return [clean_str(x.decode().strip()) for x in tqdm(f.readlines())]\n",
    "        else:\n",
    "            print(\"Loading toy data\")\n",
    "            return [clean_str(x.decode().strip()) for x in tqdm(f.readlines()[:50000])]\n",
    "\n",
    "\n",
    "# build and read stored word dictionary which is used as vocabulary for the model\n",
    "def build_dict(step, toy=False,train_article_list=[],train_title_list=[]):\n",
    "    if step == \"valid\" or os.path.exists(default_path+\"word_dict.bz\"):\n",
    "        with open(default_path+\"word_dict.bz\", \"rb\") as f:\n",
    "            word_dict = pickle.load(f,compression='bz2')\n",
    "\n",
    "    elif step == \"train\":\n",
    "        words = list()\n",
    "        for sentence in tqdm(train_article_list + train_title_list):\n",
    "            for word in word_tokenize(sentence):\n",
    "                words.append(word)\n",
    "\n",
    "        word_counter = collections.Counter(words).most_common(500000)\n",
    "        word_dict = dict()\n",
    "        word_dict[\"<padding>\"] = 0\n",
    "        word_dict[\"<unk>\"] = 1\n",
    "        word_dict[\"<s>\"] = 2\n",
    "        word_dict[\"</s>\"] = 3\n",
    "        cur_len = 4\n",
    "        for word, _ in tqdm(word_counter):\n",
    "            word_dict[word] = cur_len\n",
    "            cur_len += 1\n",
    "\n",
    "        pickle.dump(word_dict, default_path+\"word_dict\",compression='bz2')\n",
    "    \n",
    "    reversed_dict = dict(zip(word_dict.values(), word_dict.keys()))\n",
    "\n",
    "    article_max_len = 250\n",
    "    summary_max_len = 15\n",
    "\n",
    "    return word_dict, reversed_dict, article_max_len, summary_max_len\n",
    "\n",
    "\n",
    "def build_dataset(step, word_dict, article_max_len, summary_max_len, toy=False,article_list=[],title_list=[]):\n",
    "    if step == \"valid\":\n",
    "        article_list = get_text_list(valid_article_path, toy)\n",
    "    elif step == \"test\":\n",
    "        article_list = get_text_list(test_article_path, toy)\n",
    "    elif step == 'train':\n",
    "        pass\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    #print(len(article_list),len(title_list))\n",
    "    x = [word_tokenize(d) for d in tqdm(article_list)]\n",
    "    x = [[word_dict.get(w, word_dict[\"<unk>\"]) for w in d] for d in tqdm(x)]\n",
    "    x = [d[:article_max_len] for d in tqdm(x)]\n",
    "    x = [d + (article_max_len - len(d)) * [word_dict[\"<padding>\"]] for d in tqdm(x)]\n",
    "    \n",
    "    if step == \"valid\":\n",
    "        return x\n",
    "    else:        \n",
    "        y = [word_tokenize(d) for d in tqdm(title_list)]\n",
    "        y = [[word_dict.get(w, word_dict[\"<unk>\"]) for w in d] for d in tqdm(y)]\n",
    "        y = [d[:(summary_max_len - 1)] for d in tqdm(y)]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def batch_iter(inputs, outputs, batch_size, num_epochs):\n",
    "    inputs = np.array(inputs)\n",
    "    outputs = np.array(outputs)\n",
    "\n",
    "    num_batches_per_epoch = (len(inputs) - 1) // batch_size + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, len(inputs))\n",
    "            yield inputs[start_index:end_index], outputs[start_index:end_index]\n",
    "\n",
    "\n",
    "def get_init_embedding(reversed_dict, embedding_size):\n",
    "    warnings.filterwarnings('error')\n",
    "    nr_vector = len(reversed_dict)\n",
    "    word_vec_list = np.zeros((nr_vector, 300), dtype='float32')\n",
    "    for i, lex in enumerate(sorted(reversed_dict.values())):\n",
    "        try:\n",
    "            word_vec_list[i] = nlp.vocab[lex].vector / nlp.vocab[lex].vector_norm\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    # Assign random vector to <s>, </s> token\n",
    "    word_vec_list[2] = np.random.normal(0, 1, embedding_size)\n",
    "    word_vec_list[3] = np.random.normal(0, 1, embedding_size)\n",
    "    warnings.filterwarnings('default')\n",
    "    return word_vec_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "chDv2_gYDww6"
   },
   "source": [
    "#### **RNN Model Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zjn9hHrjMunu"
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, reversed_dict, article_max_len, summary_max_len, args, forward_only=False):\n",
    "        self.vocabulary_size = len(reversed_dict)\n",
    "        self.embedding_size = args.embedding_size\n",
    "        self.num_hidden = args.num_hidden\n",
    "        self.num_layers = args.num_layers\n",
    "        self.learning_rate = args.learning_rate\n",
    "        self.beam_width = args.beam_width\n",
    "        if not forward_only:\n",
    "            self.keep_prob = args.keep_prob\n",
    "        else:\n",
    "            self.keep_prob = 1.0\n",
    "        self.cell = tf.nn.rnn_cell.LSTMCell\n",
    "        \n",
    "        with tf.variable_scope(\"decoder/projection\"):\n",
    "            self.projection_layer = tf.layers.Dense(self.vocabulary_size, use_bias=False)\n",
    "\n",
    "        self.batch_size = tf.placeholder(tf.int32, (), name=\"batch_size\")\n",
    "        self.X = tf.placeholder(tf.int32, [None, article_max_len])\n",
    "        self.X_len = tf.placeholder(tf.int32, [None])\n",
    "        self.decoder_input = tf.placeholder(tf.int32, [None, summary_max_len])\n",
    "        self.decoder_len = tf.placeholder(tf.int32, [None])\n",
    "        self.decoder_target = tf.placeholder(tf.int32, [None, summary_max_len])\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            if not forward_only and args.glove:\n",
    "                init_embeddings = tf.constant(get_init_embedding(reversed_dict, self.embedding_size), dtype=tf.float32)\n",
    "            else:\n",
    "                init_embeddings = tf.random_uniform([self.vocabulary_size, self.embedding_size], -1.0, 1.0)\n",
    "            self.embeddings = tf.get_variable(\"embeddings\", initializer=init_embeddings)\n",
    "            self.encoder_emb_inp = tf.transpose(tf.nn.embedding_lookup(self.embeddings, self.X), perm=[1, 0, 2])\n",
    "            self.decoder_emb_inp = tf.transpose(tf.nn.embedding_lookup(self.embeddings, self.decoder_input), perm=[1, 0, 2])\n",
    "\n",
    "        with tf.name_scope(\"encoder\"):\n",
    "            fw_cells = [self.cell(self.num_hidden) for _ in range(self.num_layers)]\n",
    "            bw_cells = [self.cell(self.num_hidden) for _ in range(self.num_layers)]\n",
    "            fw_cells = [rnn.DropoutWrapper(cell) for cell in fw_cells]\n",
    "            bw_cells = [rnn.DropoutWrapper(cell) for cell in bw_cells]\n",
    "\n",
    "            encoder_outputs, encoder_state_fw, encoder_state_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
    "                fw_cells, bw_cells, self.encoder_emb_inp,\n",
    "                sequence_length=self.X_len, time_major=True, dtype=tf.float32)\n",
    "            self.encoder_output = tf.concat(encoder_outputs, 2)\n",
    "            encoder_state_c = tf.concat((encoder_state_fw[0].c, encoder_state_bw[0].c), 1)\n",
    "            encoder_state_h = tf.concat((encoder_state_fw[0].h, encoder_state_bw[0].h), 1)\n",
    "            self.encoder_state = rnn.LSTMStateTuple(c=encoder_state_c, h=encoder_state_h)\n",
    "\n",
    "        with tf.name_scope(\"decoder\"), tf.variable_scope(\"decoder\") as decoder_scope:\n",
    "            decoder_cell = self.cell(self.num_hidden * 2)\n",
    "\n",
    "            if not forward_only:\n",
    "                attention_states = tf.transpose(self.encoder_output, [1, 0, 2])\n",
    "                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                    self.num_hidden * 2, attention_states, memory_sequence_length=self.X_len, normalize=True)\n",
    "                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,\n",
    "                                                                   attention_layer_size=self.num_hidden * 2)\n",
    "                initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size)\n",
    "                initial_state = initial_state.clone(cell_state=self.encoder_state)\n",
    "                helper = tf.contrib.seq2seq.TrainingHelper(self.decoder_emb_inp, self.decoder_len, time_major=True)\n",
    "                decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, initial_state)\n",
    "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, output_time_major=True, scope=decoder_scope)\n",
    "                self.decoder_output = outputs.rnn_output\n",
    "                self.logits = tf.transpose(\n",
    "                    self.projection_layer(self.decoder_output), perm=[1, 0, 2])\n",
    "                self.logits_reshape = tf.concat(\n",
    "                    [self.logits, tf.zeros([self.batch_size, summary_max_len - tf.shape(self.logits)[1], self.vocabulary_size])], axis=1)\n",
    "            else:\n",
    "                tiled_encoder_output = tf.contrib.seq2seq.tile_batch(\n",
    "                    tf.transpose(self.encoder_output, perm=[1, 0, 2]), multiplier=self.beam_width)\n",
    "                tiled_encoder_final_state = tf.contrib.seq2seq.tile_batch(self.encoder_state, multiplier=self.beam_width)\n",
    "                tiled_seq_len = tf.contrib.seq2seq.tile_batch(self.X_len, multiplier=self.beam_width)\n",
    "                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                    self.num_hidden * 2, tiled_encoder_output, memory_sequence_length=tiled_seq_len, normalize=True)\n",
    "                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,\n",
    "                                                                   attention_layer_size=self.num_hidden * 2)\n",
    "                initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size * self.beam_width)\n",
    "                initial_state = initial_state.clone(cell_state=tiled_encoder_final_state)\n",
    "                decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "                    cell=decoder_cell,\n",
    "                    embedding=self.embeddings,\n",
    "                    start_tokens=tf.fill([self.batch_size], tf.constant(2)),\n",
    "                    end_token=tf.constant(3),\n",
    "                    initial_state=initial_state,\n",
    "                    beam_width=self.beam_width,\n",
    "                    output_layer=self.projection_layer\n",
    "                )\n",
    "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                    decoder, output_time_major=True, maximum_iterations=summary_max_len, scope=decoder_scope)\n",
    "                self.prediction = tf.transpose(outputs.predicted_ids, perm=[1, 2, 0])\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            if not forward_only:\n",
    "                crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    logits=self.logits_reshape, labels=self.decoder_target)\n",
    "                weights = tf.sequence_mask(self.decoder_len, summary_max_len, dtype=tf.float32)\n",
    "                self.loss = tf.reduce_sum(crossent * weights / tf.cast(self.batch_size,tf.float32))\n",
    "\n",
    "                params = tf.trainable_variables()\n",
    "                gradients = tf.gradients(self.loss, params)\n",
    "                clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "                optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "                self.update = optimizer.apply_gradients(zip(clipped_gradients, params), global_step=self.global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9qA3UURvD0W9"
   },
   "source": [
    "#### **Cell to Train the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_zqt0zKcMunz"
   },
   "outputs": [],
   "source": [
    "class args:\n",
    "    pass\n",
    "\n",
    "args.num_hidden=200\n",
    "args.num_layers=3\n",
    "args.beam_width=10\n",
    "args.embedding_size=300\n",
    "args.glove = True\n",
    "\n",
    "args.learning_rate=1e-3\n",
    "args.batch_size=64\n",
    "args.num_epochs=10\n",
    "args.keep_prob = 0.8\n",
    "\n",
    "args.toy=False #\"store_true\"\n",
    "\n",
    "args.with_model=\"store_true\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "try:\n",
    "    if not os.path.exists(default_path + \"saved_model\"):\n",
    "        os.mkdir(default_path + \"saved_model\")\n",
    "    else:\n",
    "        old_model_checkpoint_path = open(default_path + 'saved_model/checkpoint', 'r')\n",
    "        old_model_checkpoint_path = \"\".join([old_model_checkpoint_path.read().splitlines()[0].split('\"')[1] ])\n",
    "except:\n",
    "    pass\n",
    "print('Reading Training dataset...')\n",
    "try:\n",
    "    if len(train_article_list)>0:\n",
    "        pass\n",
    "except:\n",
    "    train_article_list = get_text_list(train_article_path, args.toy)\n",
    "try:\n",
    "    if len(train_title_list)>0:\n",
    "        pass\n",
    "except:\n",
    "    train_title_list = get_text_list(train_title_path, args.toy)\n",
    "\n",
    "print(\"Building dictionary...\")\n",
    "word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"train\", args.toy,train_article_list,train_title_list)\n",
    "print(\"Loading dataset...\")\n",
    "try:\n",
    "    if len(train_x)>0 and len(train_y)>0:\n",
    "        pass\n",
    "except:\n",
    "    train_x, train_y = build_dataset(\"train\", word_dict, article_max_len, summary_max_len, args.toy,train_article_list,train_title_list)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "print(\"Training the model...\")\n",
    "start = time.perf_counter()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    model = Model(reversed_dict, article_max_len, summary_max_len, args)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver(tf.global_variables(),max_to_keep=1)\n",
    "    if 'old_model_checkpoint_path' in globals():\n",
    "        print(\"Continuing from previous trained model:\" , old_model_checkpoint_path , \"...\")\n",
    "        saver.restore(sess, old_model_checkpoint_path )\n",
    "\n",
    "    batches = batch_iter(train_x, train_y, args.batch_size, args.num_epochs)\n",
    "    num_batches_per_epoch = (len(train_x) - 1) // args.batch_size + 1\n",
    "    print(\"Iteration starts.\")\n",
    "    print(\"Number of batches per epoch :\", num_batches_per_epoch)\n",
    "    t = tqdm(batches,total = num_batches_per_epoch*args.num_epochs)\n",
    "    for batch_x, batch_y in t:\n",
    "        batch_x_len = list(map(lambda x: len([y for y in x if y != 0]), batch_x))\n",
    "        batch_decoder_input = list(map(lambda x: [word_dict[\"<s>\"]] + list(x), batch_y))\n",
    "        batch_decoder_len = list(map(lambda x: len([y for y in x if y != 0]), batch_decoder_input))\n",
    "        batch_decoder_output = list(map(lambda x: list(x) + [word_dict[\"</s>\"]], batch_y))\n",
    "\n",
    "        batch_decoder_input = list(\n",
    "            map(lambda d: d + (summary_max_len - len(d)) * [word_dict[\"<padding>\"]], batch_decoder_input))\n",
    "        batch_decoder_output = list(\n",
    "            map(lambda d: d + (summary_max_len - len(d)) * [word_dict[\"<padding>\"]], batch_decoder_output))\n",
    "\n",
    "        train_feed_dict = {\n",
    "            model.batch_size: len(batch_x),\n",
    "            model.X: batch_x,\n",
    "            model.X_len: batch_x_len,\n",
    "            model.decoder_input: batch_decoder_input,\n",
    "            model.decoder_len: batch_decoder_len,\n",
    "            model.decoder_target: batch_decoder_output\n",
    "        }\n",
    "\n",
    "        _, step, loss = sess.run([model.update, model.global_step, model.loss], feed_dict=train_feed_dict)\n",
    "        cur_epoch,cur_batch = divmod(step,num_batches_per_epoch)\n",
    "        t.set_description(\"Loss:{:.3f}, Batch:{}, Epoch:{}\".format(loss,cur_batch,cur_epoch))\n",
    "\n",
    "        if step % (num_batches_per_epoch//2) == 0 and (step // (num_batches_per_epoch//2))%2 == 1:\n",
    "        #if step % 1000 == 0:\n",
    "            saver.save(sess, default_path + \"saved_model/model.ckpt\")\n",
    "            print('Model Saved. Step:{}, Loss:{:.4f}'.format(step,loss))\n",
    "\n",
    "        if step % num_batches_per_epoch == 0:\n",
    "            hours, rem = divmod(time.perf_counter() - start, 3600)\n",
    "            minutes, seconds = divmod(rem, 60)\n",
    "            saver.save(sess, default_path + \"saved_model/model.ckpt\")\n",
    "            print(\" Epoch {0}: Model is saved.\".format(step // num_batches_per_epoch),\n",
    "            \"Elapsed: {:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds) ,\n",
    "            \"Loss: {:.4f}\".format(loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ls2758geD3HY"
   },
   "source": [
    "#### **Cell for Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0xW3IJh0Mun4"
   },
   "outputs": [],
   "source": [
    "class args:\n",
    "    pass\n",
    "  \n",
    "args.num_hidden=200\n",
    "args.num_layers=3\n",
    "args.beam_width=10\n",
    "args.embedding_size=300\n",
    "args.glove = True\n",
    "\n",
    "args.learning_rate=1e-3\n",
    "args.batch_size=128\n",
    "args.num_epochs=5\n",
    "args.keep_prob = 0.8\n",
    "\n",
    "args.toy=False\n",
    "\n",
    "args.with_model=\"store_true\"\n",
    "\n",
    "\n",
    "\n",
    "print(\"Loading dictionary...\")\n",
    "word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"valid\", args.toy)\n",
    "print(\"Loading validation dataset...\")\n",
    "try:\n",
    "    if len(valid_x)>0 and len(valid_x_len)>0:\n",
    "        pass \n",
    "except:\n",
    "    valid_x = build_dataset(\"valid\", word_dict, article_max_len, summary_max_len, args.toy)\n",
    "    valid_x_len = [len([y for y in x if y != 0]) for x in valid_x]\n",
    "print(\"Loading article and reference...\")\n",
    "try:\n",
    "    if len(article)>0 and len(reference)>0:\n",
    "        pass\n",
    "except:\n",
    "    article = get_text_list(valid_article_path, args.toy)\n",
    "    reference = get_text_list(valid_title_path, args.toy)\n",
    "\n",
    "with open(default_path + \"original.txt\", \"w\") as f:\n",
    "        f.write('\\n'.join(reference))\n",
    "with open(default_path + \"result.txt\", \"w\") as f:\n",
    "        f.write('')\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"Loading saved model...\")\n",
    "    model = Model(reversed_dict, article_max_len, summary_max_len, args, forward_only=True)\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    ckpt = tf.train.get_checkpoint_state(default_path + \"saved_model/\")\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "    batches = batch_iter(valid_x, [0] * len(valid_x), args.batch_size, 1)\n",
    "\n",
    "    print(\"\\nWriting summaries to 'result.txt'...\")\n",
    "    \n",
    "    for batch_x, _ in tqdm(batches,total=len(valid_x)//args.batch_size+1):\n",
    "        batch_x_len = [len([y for y in x if y != 0]) for x in batch_x]\n",
    "\n",
    "        valid_feed_dict = {\n",
    "            model.batch_size: len(batch_x),\n",
    "            model.X: batch_x,\n",
    "            model.X_len: batch_x_len,\n",
    "        }\n",
    "\n",
    "        prediction = sess.run(model.prediction, feed_dict=valid_feed_dict)\n",
    "        prediction_output = [[reversed_dict[y] for y in x] for x in prediction[:, 0, :]]\n",
    "        summary_array = []\n",
    "        with open(default_path + \"result.txt\", \"a\") as f:\n",
    "            for line in prediction_output:\n",
    "                summary = list()\n",
    "                for word in line:\n",
    "                    if word == \"</s>\":\n",
    "                        break\n",
    "                    if word not in summary:\n",
    "                        summary.append(word)\n",
    "                summary_array.append(\" \".join(summary))\n",
    "                print(\" \".join(summary), file=f)\n",
    "    print('\\nTitles have been generated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M4qjrYnPD5iR"
   },
   "source": [
    "##### **BLEU** and **Rouge** scores calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S4hn539RqTYT"
   },
   "outputs": [],
   "source": [
    "rouge = Rouge()\n",
    "original_title,generated_title= [],[]\n",
    "print(\"Loading Data...\")\n",
    "with open(default_path + 'original.txt','r') as f:\n",
    "    original_title = list(map(str.strip,f.readlines()))\n",
    "with open(default_path + 'result.txt','r') as f:\n",
    "    generated_title = list(map(str.strip,f.readlines()))\n",
    "with bz2.open (valid_article_path, \"r\") as f:\n",
    "    abstract = list(map(str.strip,list(map(bytes.decode,f.readlines()[:len(generated_title)]))))\n",
    "\n",
    "print('Tokenizing Data...')\n",
    "tokens_original = [[word_tokenize(s)] for s in tqdm(original_title)]\n",
    "tokens_generated = [word_tokenize(s) for s in tqdm(generated_title)]\n",
    "token_abstract = [word_tokenize(s) for s in tqdm(abstract)]\n",
    "\n",
    "minmized_abstract = []\n",
    "for line in token_abstract:\n",
    "    minmized_abstract.append(' '.join(line[:40])+'...')\n",
    "\n",
    "smoothing = SmoothingFunction().method0\n",
    "print('Calculating BLEU Score')\n",
    "bleu_score = []\n",
    "for i in trange(len(tokens_original)):\n",
    "    bleu_score.append(sentence_bleu(tokens_original[i],tokens_generated[i],smoothing_function=smoothing))\n",
    "bleu = np.array(bleu_score)\n",
    "print(\"BLEU score report\")\n",
    "print(\"Min Score:\",bleu.min(),\"Max Score:\",bleu.max(),\"Avg Score:\",bleu.mean())\n",
    "\n",
    "print('Calculating Rouge Score')\n",
    "rouge1f,rouge1p,rouge1r = [],[],[]\n",
    "rouge2f,rouge2p,rouge2r = [],[],[]\n",
    "rougelf,rougelp,rougelr = [],[],[]\n",
    "for i in trange(len(tokens_original)):\n",
    "    score = rouge.get_scores(original_title[i],generated_title[i])\n",
    "    rouge1f.append(score[0]['rouge-1']['f'])\n",
    "    rouge1p.append(score[0]['rouge-1']['p'])\n",
    "    rouge1r.append(score[0]['rouge-1']['r'])\n",
    "    rouge2f.append(score[0]['rouge-2']['f'])\n",
    "    rouge2p.append(score[0]['rouge-2']['p'])\n",
    "    rouge2r.append(score[0]['rouge-2']['r'])\n",
    "    rougelf.append(score[0]['rouge-l']['f'])\n",
    "    rougelp.append(score[0]['rouge-l']['p'])\n",
    "    rougelr.append(score[0]['rouge-l']['r'])\n",
    "\n",
    "rouge1f,rouge1p,rouge1r = np.array(rouge1f),np.array(rouge1p),np.array(rouge1r)\n",
    "rouge2f,rouge2p,rouge2r = np.array(rouge2f),np.array(rouge2p),np.array(rouge2r)\n",
    "rougelf,rougelp,rougelr = np.array(rougelf),np.array(rougelp),np.array(rougelr)\n",
    "\n",
    "df = pd.DataFrame(zip(minmized_abstract,original_title,generated_title,bleu,rouge1f,rouge1p,rouge1r,rouge2f,rouge2p,rouge2r,rougelf,rougelp,rougelr),columns=['Abstract','Original_Headline','Generated_Headline_x','Bleu_Score_x','Rouge-1_F_x','Rouge-1_P_x','Rouge-1_R_x','Rouge-2_F_x','Rouge-2_P_x','Rouge-2_R_x','Rouge-l_F_x','Rouge-l_P_x','Rouge-l_R_x'])\n",
    "df.to_csv(default_path+'output_with_attention.csv',index=False)\n",
    "\n",
    "print('Done!!')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "2CceDEgqSs4Z"
   ],
   "machine_shape": "hm",
   "name": "Headline_Generation.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
