{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Headlines for Test Dataset using Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Downloading and Importing required libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install compress-pickle\n",
    "!pip install rouge\n",
    "!python -m spacy download en_core_web_md\n",
    "!sudo apt install openjdk-8-jdk\n",
    "!sudo update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
    "!pip install language-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu,SmoothingFunction\n",
    "from rouge import Rouge \n",
    "\n",
    "import collections\n",
    "import compress_pickle as pickle\n",
    "import re\n",
    "import bz2\n",
    "import os \n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import trange,tqdm\n",
    "\n",
    "import spacy\n",
    "import en_core_web_md\n",
    "\n",
    "nltk.download('punkt')\n",
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Necessary Utility functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_path = \"/Testing/\"\n",
    "dataset_path = \"/Dataset/\"\n",
    "\n",
    "test_article_path = dataset_path + \"abstract.test.bz2\"\n",
    "test_title_path   = dataset_path + \"title.test.bz2\"\n",
    "\n",
    "def clean_str(sentence):\n",
    "    sentence = re.sub(\"[#.]+\", \" \", sentence)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def get_text_list(data_path, toy=False,clean=True):\n",
    "    with bz2.open (data_path, \"r\") as f:\n",
    "        if not clean:\n",
    "            return [x.decode().strip() for x in f.readlines()[5000:10000:5]]\n",
    "        if not toy:\n",
    "            return [clean_str(x.decode().strip()) for x in tqdm(f.readlines())]\n",
    "        else:\n",
    "            return [clean_str(x.decode().strip()) for x in tqdm(f.readlines()[:20000])]\n",
    "\n",
    "\n",
    "def build_dict(step, toy=False,train_article_list=[],train_title_list=[]):\n",
    "    if step == \"test\" or os.path.exists(default_path+\"word_dict.bz\"):\n",
    "        with open(default_path+\"word_dict.bz\", \"rb\") as f:\n",
    "            word_dict = pickle.load(f,compression='bz2')\n",
    "\n",
    "    elif step == \"train\":\n",
    "        words = list()\n",
    "        for sentence in tqdm(train_article_list + train_title_list):\n",
    "            for word in word_tokenize(sentence):\n",
    "                words.append(word)\n",
    "\n",
    "        word_counter = collections.Counter(words).most_common(500000)\n",
    "        word_dict = dict()\n",
    "        word_dict[\"<padding>\"] = 0\n",
    "        word_dict[\"<unk>\"] = 1\n",
    "        word_dict[\"<s>\"] = 2\n",
    "        word_dict[\"</s>\"] = 3\n",
    "        cur_len = 4\n",
    "        for word, _ in tqdm(word_counter):\n",
    "            word_dict[word] = cur_len\n",
    "            cur_len += 1\n",
    "\n",
    "        pickle.dump(word_dict, default_path+\"word_dict\",compression='bz2')\n",
    "    \n",
    "    reversed_dict = dict(zip(word_dict.values(), word_dict.keys()))\n",
    "\n",
    "    article_max_len = 250\n",
    "    summary_max_len = 15\n",
    "\n",
    "    return word_dict, reversed_dict, article_max_len, summary_max_len\n",
    "\n",
    "\n",
    "def batch_iter(inputs, outputs, batch_size, num_epochs):\n",
    "    inputs = np.array(inputs)\n",
    "    outputs = np.array(outputs)\n",
    "\n",
    "    num_batches_per_epoch = (len(inputs) - 1) // batch_size + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, len(inputs))\n",
    "            yield inputs[start_index:end_index], outputs[start_index:end_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Title Modification ( OOV replacment and Grammar Check)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = language_check.LanguageTool('en-US')\n",
    "smoothing = SmoothingFunction().method0\n",
    "\n",
    "def get_unk_tokens(word_dict, article):\n",
    "    unk = defaultdict(float)\n",
    "    tokens = word_tokenize(article)\n",
    "    n = min(250,len(tokens))\n",
    "    for i,token in enumerate(tokens[:250]):\n",
    "        if token not in word_dict:\n",
    "            unk[token]+= get_weight(i,n)\n",
    "    tup = []\n",
    "    for i in unk:\n",
    "        tup.append((unk[i],i))\n",
    "    return sorted(tup[:5],reverse=True)\n",
    "\n",
    "def get_weight(index, token_len):\n",
    "    p = index/token_len\n",
    "    if(p<=0.1):\n",
    "        return 0.35 \n",
    "    if(p<=0.2):\n",
    "        return 0.3\n",
    "    if(p<=0.4):\n",
    "        return 0.2\n",
    "    if(p<=0.7):\n",
    "        return 0.1\n",
    "    return 0.05\n",
    "\n",
    "def correct(text):\n",
    "    matches = tool.check(text)\n",
    "    text = language_check.correct(text, matches)\n",
    "    return text\n",
    "\n",
    "def update_title(word_dict,article, title):\n",
    "    replace_count = 0\n",
    "    unk_list = get_unk_tokens(word_dict, article)\n",
    "    for j in range(min(title.count('<unk>'), len(unk_list))):\n",
    "        title = title.replace('<unk>', unk_list[j][1],1)\n",
    "        replace_count += 1\n",
    "    return correct(title)\n",
    "\n",
    "def calculate_bleu(title, reference):\n",
    "    title_tok,reference_tok = word_tokenize(title), [word_tokenize(reference)]\n",
    "    return sentence_bleu(reference_tok,title_tok,smoothing_function=smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **RNN Model Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, reversed_dict, article_max_len, summary_max_len, args, forward_only=False):\n",
    "        self.vocabulary_size = len(reversed_dict)\n",
    "        self.embedding_size = args.embedding_size\n",
    "        self.num_hidden = args.num_hidden\n",
    "        self.num_layers = args.num_layers\n",
    "        self.learning_rate = args.learning_rate\n",
    "        self.beam_width = args.beam_width\n",
    "        if not forward_only:\n",
    "            self.keep_prob = args.keep_prob\n",
    "        else:\n",
    "            self.keep_prob = 1.0\n",
    "        self.cell = tf.nn.rnn_cell.LSTMCell\n",
    "        \n",
    "        with tf.variable_scope(\"decoder/projection\"):\n",
    "            self.projection_layer = tf.layers.Dense(self.vocabulary_size, use_bias=False)\n",
    "\n",
    "        self.batch_size = tf.placeholder(tf.int32, (), name=\"batch_size\")\n",
    "        self.X = tf.placeholder(tf.int32, [None, article_max_len])\n",
    "        self.X_len = tf.placeholder(tf.int32, [None])\n",
    "        self.decoder_input = tf.placeholder(tf.int32, [None, summary_max_len])\n",
    "        self.decoder_len = tf.placeholder(tf.int32, [None])\n",
    "        self.decoder_target = tf.placeholder(tf.int32, [None, summary_max_len])\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            if not forward_only and args.glove:\n",
    "                init_embeddings = tf.constant(get_init_embedding(reversed_dict, self.embedding_size), dtype=tf.float32)\n",
    "            else:\n",
    "                init_embeddings = tf.random_uniform([self.vocabulary_size, self.embedding_size], -1.0, 1.0)\n",
    "            self.embeddings = tf.get_variable(\"embeddings\", initializer=init_embeddings)\n",
    "            self.encoder_emb_inp = tf.transpose(tf.nn.embedding_lookup(self.embeddings, self.X), perm=[1, 0, 2])\n",
    "            self.decoder_emb_inp = tf.transpose(tf.nn.embedding_lookup(self.embeddings, self.decoder_input), perm=[1, 0, 2])\n",
    "\n",
    "        with tf.name_scope(\"encoder\"):\n",
    "            fw_cells = [self.cell(self.num_hidden) for _ in range(self.num_layers)]\n",
    "            bw_cells = [self.cell(self.num_hidden) for _ in range(self.num_layers)]\n",
    "            fw_cells = [rnn.DropoutWrapper(cell) for cell in fw_cells]\n",
    "            bw_cells = [rnn.DropoutWrapper(cell) for cell in bw_cells]\n",
    "\n",
    "            encoder_outputs, encoder_state_fw, encoder_state_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
    "                fw_cells, bw_cells, self.encoder_emb_inp,\n",
    "                sequence_length=self.X_len, time_major=True, dtype=tf.float32)\n",
    "            self.encoder_output = tf.concat(encoder_outputs, 2)\n",
    "            encoder_state_c = tf.concat((encoder_state_fw[0].c, encoder_state_bw[0].c), 1)\n",
    "            encoder_state_h = tf.concat((encoder_state_fw[0].h, encoder_state_bw[0].h), 1)\n",
    "            self.encoder_state = rnn.LSTMStateTuple(c=encoder_state_c, h=encoder_state_h)\n",
    "\n",
    "        with tf.name_scope(\"decoder\"), tf.variable_scope(\"decoder\") as decoder_scope:\n",
    "            decoder_cell = self.cell(self.num_hidden * 2)\n",
    "\n",
    "            if not forward_only:\n",
    "                attention_states = tf.transpose(self.encoder_output, [1, 0, 2])\n",
    "                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                    self.num_hidden * 2, attention_states, memory_sequence_length=self.X_len, normalize=True)\n",
    "                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,\n",
    "                                                                   attention_layer_size=self.num_hidden * 2)\n",
    "                initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size)\n",
    "                initial_state = initial_state.clone(cell_state=self.encoder_state)\n",
    "                helper = tf.contrib.seq2seq.TrainingHelper(self.decoder_emb_inp, self.decoder_len, time_major=True)\n",
    "                decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, initial_state)\n",
    "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, output_time_major=True, scope=decoder_scope)\n",
    "                self.decoder_output = outputs.rnn_output\n",
    "                self.logits = tf.transpose(\n",
    "                    self.projection_layer(self.decoder_output), perm=[1, 0, 2])\n",
    "                self.logits_reshape = tf.concat(\n",
    "                    [self.logits, tf.zeros([self.batch_size, summary_max_len - tf.shape(self.logits)[1], self.vocabulary_size])], axis=1)\n",
    "            else:\n",
    "                tiled_encoder_output = tf.contrib.seq2seq.tile_batch(\n",
    "                    tf.transpose(self.encoder_output, perm=[1, 0, 2]), multiplier=self.beam_width)\n",
    "                tiled_encoder_final_state = tf.contrib.seq2seq.tile_batch(self.encoder_state, multiplier=self.beam_width)\n",
    "                tiled_seq_len = tf.contrib.seq2seq.tile_batch(self.X_len, multiplier=self.beam_width)\n",
    "                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                    self.num_hidden * 2, tiled_encoder_output, memory_sequence_length=tiled_seq_len, normalize=True)\n",
    "                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,\n",
    "                                                                   attention_layer_size=self.num_hidden * 2)\n",
    "                initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size * self.beam_width)\n",
    "                initial_state = initial_state.clone(cell_state=tiled_encoder_final_state)\n",
    "                decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "                    cell=decoder_cell,\n",
    "                    embedding=self.embeddings,\n",
    "                    start_tokens=tf.fill([self.batch_size], tf.constant(2)),\n",
    "                    end_token=tf.constant(3),\n",
    "                    initial_state=initial_state,\n",
    "                    beam_width=self.beam_width,\n",
    "                    output_layer=self.projection_layer\n",
    "                )\n",
    "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                    decoder, output_time_major=True, maximum_iterations=summary_max_len, scope=decoder_scope)\n",
    "                self.prediction = tf.transpose(outputs.predicted_ids, perm=[1, 2, 0])\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            if not forward_only:\n",
    "                crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    logits=self.logits_reshape, labels=self.decoder_target)\n",
    "                weights = tf.sequence_mask(self.decoder_len, summary_max_len, dtype=tf.float32)\n",
    "                self.loss = tf.reduce_sum(crossent * weights / tf.cast(self.batch_size,tf.float32))\n",
    "\n",
    "                params = tf.trainable_variables()\n",
    "                gradients = tf.gradients(self.loss, params)\n",
    "                clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "                optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "                self.update = optimizer.apply_gradients(zip(clipped_gradients, params), global_step=self.global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cell for Title Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    pass\n",
    "  \n",
    "args.num_hidden=200\n",
    "args.num_layers=3\n",
    "args.beam_width=10\n",
    "args.embedding_size=300\n",
    "args.glove = True\n",
    "\n",
    "args.learning_rate=1e-3\n",
    "args.batch_size=64\n",
    "args.num_epochs=5\n",
    "args.keep_prob = 0.8\n",
    "\n",
    "args.toy=True\n",
    "args.with_model=\"store_true\"\n",
    "\n",
    "word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"test\", args.toy)\n",
    "    \n",
    "def generate_title(article):\n",
    "    tf.reset_default_graph()\n",
    "    model = Model(reversed_dict, article_max_len, summary_max_len, args, forward_only=True)\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    ckpt = tf.train.get_checkpoint_state(default_path + \"saved_model/\")\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        x = [word_tokenize(clean_str(article))]\n",
    "        x = [[word_dict.get(w, word_dict[\"<unk>\"]) for w in d] for d in x]\n",
    "        x = [d[:article_max_len] for d in x]\n",
    "        test_x = [d + (article_max_len - len(d)) * [word_dict[\"<padding>\"]] for d in x]\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "            batches = batch_iter(test_x, [0] * len(test_x), args.batch_size, 1)\n",
    "\n",
    "            \n",
    "            for batch_x, _ in batches:\n",
    "                batch_x_len = [len([y for y in x if y != 0]) for x in batch_x]\n",
    "\n",
    "                test_feed_dict = {\n",
    "                    model.batch_size: len(batch_x),\n",
    "                    model.X: batch_x,\n",
    "                    model.X_len: batch_x_len,\n",
    "                }\n",
    "\n",
    "                prediction = sess.run(model.prediction, feed_dict=test_feed_dict)\n",
    "                prediction_output = [[reversed_dict[y] for y in x] for x in prediction[:, 0, :]]\n",
    "                summary_array = []\n",
    "                for line in prediction_output:\n",
    "                    summary = list()\n",
    "                    for word in line:\n",
    "                        if word == \"</s>\":\n",
    "                            break\n",
    "                        if word not in summary:\n",
    "                            summary.append(word)\n",
    "                    summary_array.append(\" \".join(summary))\n",
    "                    return \" \".join(summary)\n",
    "\n",
    "def get_title(text):\n",
    "    if text.count(' ')<10:\n",
    "        raise Exception(\"The length of the abstract is very short. Output will not be good\")\n",
    "    title = generate_title(clean_str(text))\n",
    "    updated_title = update_title(word_dict, text, title)\n",
    "    return updated_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Generate Titles for Test Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_list = get_text_list(test_article_path)\n",
    "\n",
    "generated_titles = []\n",
    "for i in trange(len(abstract_list)):\n",
    "    generated_titles.append(get_title(abstract_list[i]))\n",
    "\n",
    "with open(default_path + \"result.txt\", \"w\") as f:\n",
    "        f.write('\\n'.join(generated_titles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **BLEU** and **Rouge** scores calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = Rouge()\n",
    "original_title,generated_title= [],[]\n",
    "\n",
    "print(\"Loading Data...\")\n",
    "original_title = get_text_list(test_title_path)\n",
    "generated_title = get_generated_title(default_path + \"result.txt\")\n",
    "abstract = get_text_list(test_article_path)\n",
    "\n",
    "print('Tokenizing Data...')\n",
    "tokens_original = [[word_tokenize(s)] for s in tqdm(original_title)]\n",
    "tokens_generated = [word_tokenize(s) for s in tqdm(generated_title)]\n",
    "token_abstract = [word_tokenize(s) for s in tqdm(abstract)]\n",
    "\n",
    "minmized_abstract = []\n",
    "for line in token_abstract:\n",
    "    minmized_abstract.append(' '.join(line[:40])+'...')\n",
    "\n",
    "smoothing = SmoothingFunction().method0\n",
    "print('Calculating BLEU Score')\n",
    "bleu_score = []\n",
    "for i in trange(len(tokens_original)):\n",
    "    bleu_score.append(sentence_bleu(tokens_original[i],tokens_generated[i],smoothing_function=smoothing))\n",
    "bleu = np.array(bleu_score)\n",
    "print(\"BLEU score report\")\n",
    "print(\"Min Score:\",bleu.min(),\"Max Score:\",bleu.max(),\"Avg Score:\",bleu.mean())\n",
    "\n",
    "print('Calculating Rouge Score')\n",
    "rouge1f,rouge1p,rouge1r = [],[],[]\n",
    "rouge2f,rouge2p,rouge2r = [],[],[]\n",
    "rougelf,rougelp,rougelr = [],[],[]\n",
    "for i in trange(len(tokens_original)):\n",
    "    score = rouge.get_scores(original_title[i],generated_title[i])\n",
    "    rouge1f.append(score[0]['rouge-1']['f'])\n",
    "    rouge1p.append(score[0]['rouge-1']['p'])\n",
    "    rouge1r.append(score[0]['rouge-1']['r'])\n",
    "    rouge2f.append(score[0]['rouge-2']['f'])\n",
    "    rouge2p.append(score[0]['rouge-2']['p'])\n",
    "    rouge2r.append(score[0]['rouge-2']['r'])\n",
    "    rougelf.append(score[0]['rouge-l']['f'])\n",
    "    rougelp.append(score[0]['rouge-l']['p'])\n",
    "    rougelr.append(score[0]['rouge-l']['r'])\n",
    "\n",
    "rouge1f,rouge1p,rouge1r = np.array(rouge1f),np.array(rouge1p),np.array(rouge1r)\n",
    "rouge2f,rouge2p,rouge2r = np.array(rouge2f),np.array(rouge2p),np.array(rouge2r)\n",
    "rougelf,rougelp,rougelr = np.array(rougelf),np.array(rougelp),np.array(rougelr)\n",
    "\n",
    "df = pd.DataFrame(zip(minmized_abstract,original_title,generated_title,bleu,rouge1f,rouge1p,rouge1r,rouge2f,rouge2p,rouge2r,rougelf,rougelp,rougelr),columns=['Abstract','Original_Headline','Generated_Headline_x','Bleu_Score_x','Rouge-1_F_x','Rouge-1_P_x','Rouge-1_R_x','Rouge-2_F_x','Rouge-2_P_x','Rouge-2_R_x','Rouge-l_F_x','Rouge-l_P_x','Rouge-l_R_x'])\n",
    "df.to_csv(default_path+'output.csv',index=False)\n",
    "\n",
    "print('Done!!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
