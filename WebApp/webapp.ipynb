{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LAAIJU2XfcQK"
   },
   "source": [
    "# WebApp for Title Generation of Research Articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Downloading and Importing required libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E9KQRcuDRGr_"
   },
   "outputs": [],
   "source": [
    "!pip install compress-pickle\n",
    "!pip install rouge\n",
    "!python -m spacy download en_core_web_md\n",
    "!sudo apt install openjdk-8-jdk\n",
    "!sudo update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java\n",
    "!pip install language-check\n",
    "!pip install flask-ngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu,SmoothingFunction\n",
    "from rouge import Rouge \n",
    "\n",
    "from flask_ngrok import run_with_ngrok\n",
    "from flask import Flask, request, render_template, redirect, url_for, send_from_directory, flash\n",
    "\n",
    "import random\n",
    "import collections\n",
    "import compress_pickle as pickle\n",
    "import re\n",
    "import bz2\n",
    "import os \n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import trange,tqdm\n",
    "\n",
    "import spacy\n",
    "import en_core_web_md\n",
    "\n",
    "nltk.download('punkt')\n",
    "nlp = en_core_web_md.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1aPmW9jzfjRl"
   },
   "source": [
    "#### **Necessary Utility functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jnWPROvDSgDP"
   },
   "outputs": [],
   "source": [
    "default_path = \"/Testing/\"\n",
    "dataset_path = \"/Dataset/\"\n",
    "\n",
    "test_article_path = dataset_path + \"abstract.test.bz2\"\n",
    "test_title_path   = dataset_path + \"title.test.bz2\"\n",
    "\n",
    "def clean_str(sentence):\n",
    "    sentence = re.sub(\"[#.]+\", \" \", sentence)\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def get_text_list(data_path, toy=False,clean=True):\n",
    "    with bz2.open (data_path, \"r\") as f:\n",
    "        if not clean:\n",
    "            return [x.decode().strip() for x in f.readlines()[5000:10000:5]]\n",
    "        if not toy:\n",
    "            return [clean_str(x.decode().strip()) for x in tqdm(f.readlines())]\n",
    "        else:\n",
    "            return [clean_str(x.decode().strip()) for x in tqdm(f.readlines()[:20000])]\n",
    "\n",
    "\n",
    "def build_dict(step, toy=False,train_article_list=[],train_title_list=[]):\n",
    "    if step == \"test\" or os.path.exists(default_path+\"word_dict.bz\"):\n",
    "        with open(default_path+\"word_dict.bz\", \"rb\") as f:\n",
    "            word_dict = pickle.load(f,compression='bz2')\n",
    "\n",
    "    elif step == \"train\":\n",
    "        words = list()\n",
    "        for sentence in tqdm(train_article_list + train_title_list):\n",
    "            for word in word_tokenize(sentence):\n",
    "                words.append(word)\n",
    "\n",
    "        word_counter = collections.Counter(words).most_common(500000)\n",
    "        word_dict = dict()\n",
    "        word_dict[\"<padding>\"] = 0\n",
    "        word_dict[\"<unk>\"] = 1\n",
    "        word_dict[\"<s>\"] = 2\n",
    "        word_dict[\"</s>\"] = 3\n",
    "        cur_len = 4\n",
    "        for word, _ in tqdm(word_counter):\n",
    "            word_dict[word] = cur_len\n",
    "            cur_len += 1\n",
    "\n",
    "        pickle.dump(word_dict, default_path+\"word_dict\",compression='bz2')\n",
    "    \n",
    "    reversed_dict = dict(zip(word_dict.values(), word_dict.keys()))\n",
    "\n",
    "    article_max_len = 250\n",
    "    summary_max_len = 15\n",
    "\n",
    "    return word_dict, reversed_dict, article_max_len, summary_max_len\n",
    "\n",
    "\n",
    "def batch_iter(inputs, outputs, batch_size, num_epochs):\n",
    "    inputs = np.array(inputs)\n",
    "    outputs = np.array(outputs)\n",
    "\n",
    "    num_batches_per_epoch = (len(inputs) - 1) // batch_size + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, len(inputs))\n",
    "            yield inputs[start_index:end_index], outputs[start_index:end_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jgkXbFwffnxf"
   },
   "source": [
    "#### **Title Modification ( OOV replacment and Grammar Check)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hmRRxc-ucQZn"
   },
   "outputs": [],
   "source": [
    "tool = language_check.LanguageTool('en-US')\n",
    "smoothing = SmoothingFunction().method0\n",
    "\n",
    "def get_unk_tokens(word_dict, article):\n",
    "    unk = defaultdict(float)\n",
    "    tokens = word_tokenize(article)\n",
    "    n = min(250,len(tokens))\n",
    "    for i,token in enumerate(tokens[:250]):\n",
    "        if token not in word_dict:\n",
    "            unk[token]+= get_weight(i,n)\n",
    "    tup = []\n",
    "    for i in unk:\n",
    "        tup.append((unk[i],i))\n",
    "    return sorted(tup[:5],reverse=True)\n",
    "\n",
    "def get_weight(index, token_len):\n",
    "    p = index/token_len\n",
    "    if(p<=0.1):\n",
    "        return 0.35 \n",
    "    if(p<=0.2):\n",
    "        return 0.3\n",
    "    if(p<=0.4):\n",
    "        return 0.2\n",
    "    if(p<=0.7):\n",
    "        return 0.1\n",
    "    return 0.05\n",
    "\n",
    "def correct(text):\n",
    "    matches = tool.check(text)\n",
    "    text = language_check.correct(text, matches)\n",
    "    return text\n",
    "\n",
    "def update_title(word_dict,article, title):\n",
    "    replace_count = 0\n",
    "    unk_list = get_unk_tokens(word_dict, article)\n",
    "    for j in range(min(title.count('<unk>'), len(unk_list))):\n",
    "        title = title.replace('<unk>', unk_list[j][1],1)\n",
    "        replace_count += 1\n",
    "    return correct(title)\n",
    "\n",
    "def calculate_bleu(title, reference):\n",
    "    title_tok,reference_tok = word_tokenize(title), [word_tokenize(reference)]\n",
    "    return sentence_bleu(reference_tok,title_tok,smoothing_function=smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y2QYkJUsfxin"
   },
   "source": [
    "#### **RNN Model Implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JNyFTy_Qfa2g"
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, reversed_dict, article_max_len, summary_max_len, args, forward_only=False):\n",
    "        self.vocabulary_size = len(reversed_dict)\n",
    "        self.embedding_size = args.embedding_size\n",
    "        self.num_hidden = args.num_hidden\n",
    "        self.num_layers = args.num_layers\n",
    "        self.learning_rate = args.learning_rate\n",
    "        self.beam_width = args.beam_width\n",
    "        if not forward_only:\n",
    "            self.keep_prob = args.keep_prob\n",
    "        else:\n",
    "            self.keep_prob = 1.0\n",
    "        self.cell = tf.nn.rnn_cell.LSTMCell\n",
    "        \n",
    "        with tf.variable_scope(\"decoder/projection\"):\n",
    "            self.projection_layer = tf.layers.Dense(self.vocabulary_size, use_bias=False)\n",
    "\n",
    "        self.batch_size = tf.placeholder(tf.int32, (), name=\"batch_size\")\n",
    "        self.X = tf.placeholder(tf.int32, [None, article_max_len])\n",
    "        self.X_len = tf.placeholder(tf.int32, [None])\n",
    "        self.decoder_input = tf.placeholder(tf.int32, [None, summary_max_len])\n",
    "        self.decoder_len = tf.placeholder(tf.int32, [None])\n",
    "        self.decoder_target = tf.placeholder(tf.int32, [None, summary_max_len])\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            if not forward_only and args.glove:\n",
    "                init_embeddings = tf.constant(get_init_embedding(reversed_dict, self.embedding_size), dtype=tf.float32)\n",
    "            else:\n",
    "                init_embeddings = tf.random_uniform([self.vocabulary_size, self.embedding_size], -1.0, 1.0)\n",
    "            self.embeddings = tf.get_variable(\"embeddings\", initializer=init_embeddings)\n",
    "            self.encoder_emb_inp = tf.transpose(tf.nn.embedding_lookup(self.embeddings, self.X), perm=[1, 0, 2])\n",
    "            self.decoder_emb_inp = tf.transpose(tf.nn.embedding_lookup(self.embeddings, self.decoder_input), perm=[1, 0, 2])\n",
    "\n",
    "        with tf.name_scope(\"encoder\"):\n",
    "            fw_cells = [self.cell(self.num_hidden) for _ in range(self.num_layers)]\n",
    "            bw_cells = [self.cell(self.num_hidden) for _ in range(self.num_layers)]\n",
    "            fw_cells = [rnn.DropoutWrapper(cell) for cell in fw_cells]\n",
    "            bw_cells = [rnn.DropoutWrapper(cell) for cell in bw_cells]\n",
    "\n",
    "            encoder_outputs, encoder_state_fw, encoder_state_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
    "                fw_cells, bw_cells, self.encoder_emb_inp,\n",
    "                sequence_length=self.X_len, time_major=True, dtype=tf.float32)\n",
    "            self.encoder_output = tf.concat(encoder_outputs, 2)\n",
    "            encoder_state_c = tf.concat((encoder_state_fw[0].c, encoder_state_bw[0].c), 1)\n",
    "            encoder_state_h = tf.concat((encoder_state_fw[0].h, encoder_state_bw[0].h), 1)\n",
    "            self.encoder_state = rnn.LSTMStateTuple(c=encoder_state_c, h=encoder_state_h)\n",
    "\n",
    "        with tf.name_scope(\"decoder\"), tf.variable_scope(\"decoder\") as decoder_scope:\n",
    "            decoder_cell = self.cell(self.num_hidden * 2)\n",
    "\n",
    "            if not forward_only:\n",
    "                attention_states = tf.transpose(self.encoder_output, [1, 0, 2])\n",
    "                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                    self.num_hidden * 2, attention_states, memory_sequence_length=self.X_len, normalize=True)\n",
    "                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,\n",
    "                                                                   attention_layer_size=self.num_hidden * 2)\n",
    "                initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size)\n",
    "                initial_state = initial_state.clone(cell_state=self.encoder_state)\n",
    "                helper = tf.contrib.seq2seq.TrainingHelper(self.decoder_emb_inp, self.decoder_len, time_major=True)\n",
    "                decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper, initial_state)\n",
    "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder, output_time_major=True, scope=decoder_scope)\n",
    "                self.decoder_output = outputs.rnn_output\n",
    "                self.logits = tf.transpose(\n",
    "                    self.projection_layer(self.decoder_output), perm=[1, 0, 2])\n",
    "                self.logits_reshape = tf.concat(\n",
    "                    [self.logits, tf.zeros([self.batch_size, summary_max_len - tf.shape(self.logits)[1], self.vocabulary_size])], axis=1)\n",
    "            else:\n",
    "                tiled_encoder_output = tf.contrib.seq2seq.tile_batch(\n",
    "                    tf.transpose(self.encoder_output, perm=[1, 0, 2]), multiplier=self.beam_width)\n",
    "                tiled_encoder_final_state = tf.contrib.seq2seq.tile_batch(self.encoder_state, multiplier=self.beam_width)\n",
    "                tiled_seq_len = tf.contrib.seq2seq.tile_batch(self.X_len, multiplier=self.beam_width)\n",
    "                attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n",
    "                    self.num_hidden * 2, tiled_encoder_output, memory_sequence_length=tiled_seq_len, normalize=True)\n",
    "                decoder_cell = tf.contrib.seq2seq.AttentionWrapper(decoder_cell, attention_mechanism,\n",
    "                                                                   attention_layer_size=self.num_hidden * 2)\n",
    "                initial_state = decoder_cell.zero_state(dtype=tf.float32, batch_size=self.batch_size * self.beam_width)\n",
    "                initial_state = initial_state.clone(cell_state=tiled_encoder_final_state)\n",
    "                decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "                    cell=decoder_cell,\n",
    "                    embedding=self.embeddings,\n",
    "                    start_tokens=tf.fill([self.batch_size], tf.constant(2)),\n",
    "                    end_token=tf.constant(3),\n",
    "                    initial_state=initial_state,\n",
    "                    beam_width=self.beam_width,\n",
    "                    output_layer=self.projection_layer\n",
    "                )\n",
    "                outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "                    decoder, output_time_major=True, maximum_iterations=summary_max_len, scope=decoder_scope)\n",
    "                self.prediction = tf.transpose(outputs.predicted_ids, perm=[1, 2, 0])\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            if not forward_only:\n",
    "                crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    logits=self.logits_reshape, labels=self.decoder_target)\n",
    "                weights = tf.sequence_mask(self.decoder_len, summary_max_len, dtype=tf.float32)\n",
    "                self.loss = tf.reduce_sum(crossent * weights / tf.cast(self.batch_size,tf.float32))\n",
    "\n",
    "                params = tf.trainable_variables()\n",
    "                gradients = tf.gradients(self.loss, params)\n",
    "                clipped_gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "                optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "                self.update = optimizer.apply_gradients(zip(clipped_gradients, params), global_step=self.global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uw93gijxf2Bc"
   },
   "source": [
    "#### **Cell for Title Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1RfHOeNjX97Z"
   },
   "outputs": [],
   "source": [
    "class args:\n",
    "    pass\n",
    "  \n",
    "args.num_hidden=200\n",
    "args.num_layers=3\n",
    "args.beam_width=10\n",
    "args.embedding_size=300\n",
    "args.glove = True\n",
    "\n",
    "args.learning_rate=1e-3\n",
    "args.batch_size=64\n",
    "args.num_epochs=5\n",
    "args.keep_prob = 0.8\n",
    "\n",
    "args.toy=True\n",
    "args.with_model=\"store_true\"\n",
    "\n",
    "word_dict, reversed_dict, article_max_len, summary_max_len = build_dict(\"test\", args.toy)\n",
    "abstracts = get_text_list(test_article_path)\n",
    "titles = get_text_list(test_title_path)\n",
    "    \n",
    "def generate_title(article):\n",
    "    tf.reset_default_graph()\n",
    "    model = Model(reversed_dict, article_max_len, summary_max_len, args, forward_only=True)\n",
    "    saver = tf.train.Saver(tf.global_variables())\n",
    "    ckpt = tf.train.get_checkpoint_state(default_path + \"saved_model/\")\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        x = [word_tokenize(clean_str(article))]\n",
    "        x = [[word_dict.get(w, word_dict[\"<unk>\"]) for w in d] for d in x]\n",
    "        x = [d[:article_max_len] for d in x]\n",
    "        test_x = [d + (article_max_len - len(d)) * [word_dict[\"<padding>\"]] for d in x]\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "\n",
    "            batches = batch_iter(test_x, [0] * len(test_x), args.batch_size, 1)\n",
    "\n",
    "            \n",
    "            for batch_x, _ in batches:\n",
    "                batch_x_len = [len([y for y in x if y != 0]) for x in batch_x]\n",
    "\n",
    "                test_feed_dict = {\n",
    "                    model.batch_size: len(batch_x),\n",
    "                    model.X: batch_x,\n",
    "                    model.X_len: batch_x_len,\n",
    "                }\n",
    "\n",
    "                prediction = sess.run(model.prediction, feed_dict=test_feed_dict)\n",
    "                prediction_output = [[reversed_dict[y] for y in x] for x in prediction[:, 0, :]]\n",
    "                summary_array = []\n",
    "                for line in prediction_output:\n",
    "                    summary = list()\n",
    "                    for word in line:\n",
    "                        if word == \"</s>\":\n",
    "                            break\n",
    "                        if word not in summary:\n",
    "                            summary.append(word)\n",
    "                    summary_array.append(\" \".join(summary))\n",
    "                    return \" \".join(summary)\n",
    "\n",
    "def get_title(text):\n",
    "    if text.count(' ')<10:\n",
    "        raise Exception(\"The length of the abstract is very short. Output will not be good\")\n",
    "    title = generate_title(clean_str(text))\n",
    "    updated_title, unk_list, replace_count = update_title(word_dict, text, title)\n",
    "    result = {}\n",
    "    result['text'] = text[:700]+'...'\n",
    "    result['title'] = title\n",
    "    result['updated_title'] = updated_title\n",
    "    result['unk_list'] = unk_list\n",
    "    result['replace_count'] = replace_count\n",
    "    return result\n",
    "\n",
    "\n",
    "def evaluate_title(text,title):\n",
    "    title_gen = get_title(text)\n",
    "    bleu_score = calculate_bleu(title_gen['updated_title'], title)\n",
    "    result = {}\n",
    "    result['text'] = text[:700]+'...'\n",
    "    result['original'] = title\n",
    "    result['generated'] = title_gen['updated_title']\n",
    "    result['bleu'] = bleu_score\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **Cell for WebApp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2wpkWToBfBWV"
   },
   "outputs": [],
   "source": [
    "app = Flask(__name__,template_folder='drive/My Drive/templates')\n",
    "app.config['N'] = 0\n",
    "\n",
    "app.config['SECRET_KEY'] = '5791628bb0b13ce0c676dfde280ba245'\n",
    "run_with_ngrok(app)   #starts ngrok when the app is run\n",
    "\n",
    "\n",
    "@app.route('/', methods=['GET', \"POST\"])\n",
    "def home():\n",
    "    if request.method == 'POST':\n",
    "        abstract = request.form['text']\n",
    "\n",
    "        # call predict\n",
    "        # something like\n",
    "        # result , unklist = model.predct(abstract = abstract)\n",
    "        time_taken = time.time()\n",
    "        try:\n",
    "          response = get_title(abstract)\n",
    "        except:\n",
    "          response = None\n",
    "        title = abstract\n",
    "        time_taken = str(round(time.time() - time_taken, 3)) + \" secs.\"\n",
    "        unklist = ['test','a']\n",
    "        if(response):\n",
    "          result = {'time_taken': time_taken,\n",
    "                    'title': response['updated_title'], 'unk-list': response['unk_list'],'replace-count' : response['replace_count']}\n",
    "          return render_template('home2.html', abstract=abstract, result=result)\n",
    "        else:\n",
    "          flash('Too small Input for Title Generation','danger')\n",
    "          return redirect(url_for(\"home\"))\n",
    "    return render_template('home2.html')\n",
    "\n",
    "\n",
    "@app.route(\"/clear\")\n",
    "def clear():\n",
    "    return render_template('home2.html')\n",
    "\n",
    "\n",
    "@app.route(\"/random_sample/<route>\")\n",
    "def random_sample(route):\n",
    "    app.config['N'] = random.randrange(len(abstracts))\n",
    "    random_abstract = abstracts[app.config['N']]\n",
    "    random_title = titles[app.config['N']]\n",
    "    if(route == \"evaluate\"):\n",
    "        return render_template(\"evaluate2.html\", abstract=random_abstract, input_title = random_title)\n",
    "    else:\n",
    "        return render_template(\"home2.html\", abstract=random_abstract)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@app.route(\"/evaluate\", methods=['GET', \"POST\"])\n",
    "def evaluate():\n",
    "    if request.method == 'POST':\n",
    "        abstract = request.form['asbtract']\n",
    "        input_title = request.form['title']\n",
    "        time_taken = time.time()\n",
    "        # call predict\n",
    "        # something like\n",
    "        # result , unklist = model.predct(abstract = abstract)\n",
    "        try:\n",
    "          response = evaluate_title(abstract,input_title)\n",
    "        except:\n",
    "          response = None\n",
    "        title = str(abstract)\n",
    "        time_taken = str(round(time.time() - time_taken, 3)) + \" secs.\"\n",
    "        print(time_taken)\n",
    "        bleu = None\n",
    "        if(response):\n",
    "          result = {'time_taken': time_taken, 'title': response['generated'], 'bleu': response['bleu']}\n",
    "          return render_template('evaluate2.html', abstract=abstract, input_title= input_title, result=result)\n",
    "        else:\n",
    "          flash('Too small Input for Title Generation','danger')\n",
    "          return redirect(url_for(\"evaluate\"))\n",
    "    return redirect(url_for('random_sample', route=\"evaluate\"))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # INit model here\n",
    "    app.run()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPfTbq6+kkhvd/icLL2zJbY",
   "collapsed_sections": [],
   "name": "kaustav_final.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1AS8CDFi5TXi5Jvp99mJLiUQY3vYbmNIs",
     "timestamp": 1593877718770
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
